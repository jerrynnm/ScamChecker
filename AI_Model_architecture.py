"""æµç¨‹åœ–
è®€å–è³‡æ–™ â†’ åˆ†å‰²è³‡æ–™ â†’ ç·¨ç¢¼ â†’ å»ºç«‹ Dataset / DataLoader
â†“
å»ºç«‹æ¨¡å‹ï¼ˆBERT+LSTM+CNNï¼‰
        â†“
        BERT è¼¸å‡º [batch, seq_len, 768]
        â†“
        BiLSTM  [batch, seq_len, hidden_dim*2]
        â†“
        CNN æ¨¡çµ„ (Conv1D + Dropout + GlobalMaxPooling1D)
        â†“
        Linear åˆ†é¡å™¨ï¼ˆè¼¸å‡ºè©é¨™æ©Ÿç‡ï¼‰
        â†“
è¨“ç·´æ¨¡å‹ï¼ˆEpochsï¼‰
â†“
è©•ä¼°æ¨¡å‹ï¼ˆAccuracy / F1 / Precision / Recallï¼‰
â†“
å„²å­˜æ¨¡å‹ï¼ˆ.pthï¼‰

"""#å¼•å…¥é‡è¦å¥—ä»¶Import Library
import torch                            #   PyTorch ä¸»æ¨¡çµ„               
import torch.nn as nn                   #	ç¥ç¶“ç¶²è·¯ç›¸é—œçš„å±¤ï¼ˆä¾‹å¦‚ LSTMã€Linearï¼‰
import torch.nn.functional as F         #   æä¾›ç´”å‡½å¼ç‰ˆçš„æ“ä½œæ–¹æ³•ï¼Œåƒæ˜¯ F.relu()ã€F.cross_entropy()ï¼Œé€šå¸¸ä¸å¸¶åƒæ•¸ã€ä¸è‡ªå‹•å»ºç«‹æ¬Šé‡
import numpy as np                      
import pandas as pd
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:16"#è®“ CUDA ä½¿ç”¨ã€Œæ›´å°è¨˜æ†¶é«”åˆ†é…å¡Šã€çš„æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆæ¸›å°‘ OOM éŒ¯èª¤ã€‚
import re

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset #	æä¾› Datasetã€DataLoader é¡åˆ¥
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split
from transformers import BertModel
#BertTokenizer	æŠŠæ–‡å­—å¥å­è½‰æ›æˆ BERT æ ¼å¼çš„ token IDï¼Œä¾‹å¦‚ [CLS] ä»Šå¤© å¤©æ°£ ä¸éŒ¯ [SEP] â†’ [101, 1234, 5678, ...]
##BertForSequenceClassification	æ˜¯ Hugging Face æä¾›çš„ä¸€å€‹å®Œæ•´ BERT æ¨¡å‹ï¼Œæ¥äº†åˆ†é¡ç”¨çš„ Linear å±¤ï¼Œè®“ä½ ç›´æ¥æ‹¿ä¾†åšåˆ†é¡ä»»å‹™ï¼ˆä¾‹å¦‚è©é¨™ vs æ­£å¸¸ï¼‰


#æ­£å¸¸è¨Šæ¯è³‡æ–™é›†åœ¨é€™æ–°å¢
normal_files = [r"C:\Users\user\Desktop\å°ˆæ¡ˆç¨‹å¼0527\Project_PredictScamInfo\data\NorANDScamInfo_data1.csv"]

#è©é¨™è¨Šæ¯è³‡æ–™é›†åœ¨é€™æ–°å¢
scam_files = [
    r"C:\Users\user\Desktop\å°ˆæ¡ˆç¨‹å¼0527\Project_PredictScamInfo\data\NorANDScamInfo_data1.csv"]

#è³‡æ–™å‰è™•ç†
class BertPreprocessor:
    def __init__(self, tokenizer_name="ckiplab/bert-base-chinese", max_len=128):
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)
        self.max_len = max_len

    def load_and_clean(self, filepath):
        #è¼‰å…¥ CSV ä¸¦æ¸…ç† message æ¬„ä½ã€‚
        df = pd.read_csv(filepath)
        df = df.dropna().drop_duplicates().reset_index(drop=True)
        # æ–‡å­—æ¸…ç†ï¼šç§»é™¤ç©ºç™½ã€ä¿ç•™ä¸­æ–‡è‹±æ•¸èˆ‡æ¨™é»
        df["message"] = df["message"].astype(str)
        df["message"] = df["message"].apply(lambda text: re.sub(r"\s+", "", text))
        df["message"] = df["message"].apply(lambda text: re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ]", "", text))
        return df[["message", "label"]]  # ä¿ç•™å¿…è¦æ¬„ä½

    def encode(self, messages):
        #ä½¿ç”¨ HuggingFace BERT Tokenizer å°‡è¨Šæ¯ç·¨ç¢¼æˆæ¨¡å‹è¼¸å…¥æ ¼å¼ã€‚
        return self.tokenizer(
            list(messages),
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=self.max_len
        )
#è‡ªå‹•åšè³‡æ–™å‰è™•ç†
def build_bert_inputs(normal_files, scam_files):
    #å°‡æ­£å¸¸èˆ‡è©é¨™è³‡æ–™åˆ†åˆ¥æŒ‡å®š labelï¼Œçµ±ä¸€æ¸…ç†ã€ç·¨ç¢¼ï¼Œå›å‚³æ¨¡å‹å¯ç”¨çš„ input tensors èˆ‡ labelsã€‚
    processor = BertPreprocessor()
    dfs = []
    # åˆä½µæ­£å¸¸ + è©é¨™æª”æ¡ˆæ¸…å–®
    all_files = normal_files + scam_files

    for filepath in all_files:
        df = processor.load_and_clean(filepath)
        dfs.append(df)

    # åˆä½µæ‰€æœ‰è³‡æ–™ã€‚åœ¨è³‡æ–™æ¸…ç†éç¨‹ä¸­dropna()ï¼šåˆªé™¤æœ‰ç©ºå€¼çš„åˆ—ï¼Œdrop_duplicates()ï¼šåˆªé™¤é‡è¤‡åˆ—ï¼Œfilter()æˆ–df[...]åšæ¢ä»¶éæ¿¾ï¼Œconcat():å°‡å¤šå€‹ DataFrameåˆä½µ
    # é€™äº›æ“ä½œä¸æœƒè‡ªå‹•é‡æ’ç´¢å¼•ï¼Œé€ æˆç´¢å¼•äº‚æ‰ã€‚
    # åˆä½µå¾Œçµ±ä¸€ç·¨è™Ÿï¼ˆå¸¸è¦‹æ–¼å¤šç­†è³‡æ–™åˆä½µï¼‰all_df = pd.concat(dfs, é—œéµ-->ignore_index=True)
    all_df = pd.concat(dfs, ignore_index=True)
    #è£½ä½œ train/val è³‡æ–™é›†
    train_texts, val_texts, train_labels, val_labels = train_test_split(
    all_df["message"], all_df["label"],
    stratify=all_df["label"],
    test_size=0.2,
    random_state=25,
    shuffle=True
    )
    
    # é€²è¡Œ BERT tokenizer ç·¨ç¢¼
    train_inputs = processor.encode(train_texts)
    val_inputs = processor.encode(val_texts)

    return train_inputs, train_labels, val_inputs, val_labels, processor

#AUTO YA~ä»¥forè¿´åœˆè‡ªå‹•æ–°å¢å€‹åˆ¥è®Šæ•¸å…§ï¼Œbuild_bert_inputsèƒ½è‡ªå‹•æ“·å–æ–°å¢è³‡æ–™
normal_files_labels = [normal for normal in normal_files] 
scam_files_labels = [scam for scam in scam_files] 

#print(bert_inputs.keys())

#å®šç¾© PyTorch Dataset é¡åˆ¥
class ScamDataset(Dataset):
    def __init__(self, inputs, labels):
        self.input_ids = inputs["input_ids"]                           # input_idsï¼šå¥å­çš„ token ID; attention_maskï¼šæ³¨æ„åŠ›é®ç½©ï¼ˆ0 = paddingï¼‰
        self.attention_mask = inputs["attention_mask"]                 # token_type_idsï¼šå¥å­çš„ segment å€åˆ†
        self.token_type_ids = inputs["token_type_ids"]                 # torch.tensor(x, dtype=...)å°‡è³‡æ–™(x)è½‰ç‚ºTensorçš„æ¨™æº–åšæ³•ã€‚
        self.labels = torch.tensor(labels.values, dtype=torch.float32) # xå¯ä»¥æ˜¯ listã€NumPy arrayã€pandas series...
# dtypefloat32ï¼šæµ®é»æ•¸(å¸¸ç”¨æ–¼ å›æ­¸ æˆ– BCELoss äºŒåˆ†é¡);longï¼šæ•´æ•¸(å¸¸ç”¨æ–¼ å¤šåˆ†é¡ æ­é… CrossEntropyLoss)ã€‚labels.values â†’ è½‰ç‚º NumPy array
    def __len__(self):          # å‘Šè¨´ PyTorch é€™å€‹ Dataset æœ‰å¹¾ç­†è³‡æ–™
        return len(self.labels) # çµ¦ len(dataset) æˆ– for i in range(len(dataset)) ç”¨çš„
    
    def __getitem__(self, idx): #å›å‚³ç¬¬ idx ç­†è³‡æ–™ï¼ˆæœƒè‡ªå‹•åœ¨è¨“ç·´ä¸­ä¸€ç­†ç­†æŠ“ï¼‰
        return {                #DataLoader æ¯æ¬¡æœƒå‘¼å«é€™å€‹æ–¹æ³•å¤šæ¬¡ä¾†æŠ“ä¸€å€‹ batch çš„è³‡æ–™
            "input_ids":self.input_ids[idx],
            "attention_mask":self.attention_mask[idx],
            "token_type_ids":self.token_type_ids[idx],
            "labels":self.labels[idx]
        }

# é€™æ¨£å¯ä»¥åŒæ™‚è™•ç† scam å’Œ normal è³‡æ–™ï¼Œä¸ç”¨é‡è¤‡å¯«æ¸…ç†èˆ‡ token è™•ç†
train_inputs, train_labels, val_inputs, val_labels, processor = build_bert_inputs(normal_files, scam_files)

train_dataset = ScamDataset(train_inputs, train_labels)
val_dataset = ScamDataset(val_inputs, val_labels)

train_loader = DataLoader(train_dataset, batch_size=8)
val_loader = DataLoader(val_dataset, batch_size=8)

#æ¨¡å‹
class BertLSTM_CNN_Classifier(nn.Module):
    def __init__(self, hidden_dim=128, num_layers=1, dropout=0.3):
        super(BertLSTM_CNN_Classifier, self).__init__()
        self.bert = BertModel.from_pretrained("ckiplab/bert-base-chinese") #è¼‰å…¥é è¨“ç·´ BERT æ¨¡å‹ï¼ˆckiplab ä¸­æ–‡ç‰ˆï¼‰
        # LSTM æ¥åœ¨ BERT çš„ token è¼¸å‡ºå¾Œï¼ˆè¼¸å…¥æ˜¯768ç¶­ï¼‰
        self.LSTM = nn.LSTM(input_size=768,         # æŠŠ BERT çš„ token åºåˆ—å†äº¤çµ¦é›™å‘ LSTM åšæ™‚é–“åºåˆ—å»ºæ¨¡
                            hidden_size=hidden_dim,
                            num_layers=num_layers,
                            batch_first=True,
                            bidirectional=True)
         # CNN æ¨¡çµ„ï¼šæ¥åœ¨ LSTM å¾Œçš„è¼¸å‡ºä¸Š
        self.conv1 =  nn.Conv1d(in_channels=hidden_dim*2,
                                out_channels=128,
                                kernel_size=3,
                                padding=1)
        self.dropout = nn.Dropout(dropout) 
        self.global_maxpool = nn.AdaptiveAvgPool1d(1)        # ç­‰æ•ˆæ–¼ GlobalMaxPooling1D

        self.classifier = nn.Linear(128,1)
    def forward(self, input_ids, attention_mask, token_type_ids):
        outputs = self.bert(input_ids=input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids)
        hidden_states = outputs.last_hidden_state  # [batch, seq_len, 768]

        LSTM_out, _ = self.LSTM(hidden_states)     # [batch, seq_len, hidden_dim*2]
        LSTM_out = LSTM_out.transpose(1, 2)        # [batch, hidden_dim*2, seq_len]

        x = self.conv1(LSTM_out)                   # [batch, 128, seq_len]
        x = self.dropout(x)
        x = self.global_maxpool(x).squeeze(2)      # [batch, 128]

        logits = self.classifier(x)
        return torch.sigmoid(logits).view(-1)  # ğŸ‘ˆ ä¿®æ­£é€™è¡Œ

        
# è¨­å®š GPU è£ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# è¨­å®šä½¿ç”¨çš„æœ€å¤§åŸ·è¡Œç·’æ•¸ï¼ˆè¦– CPU è€Œå®šï¼‰
torch.set_num_threads(8)  # å»ºè­°è¨­æˆä½ ç³»çµ±çš„å¯¦é«”æ ¸å¿ƒæ•¸
# åˆå§‹åŒ–æ¨¡å‹
model = BertLSTM_CNN_Classifier().to(device)
# å®šç¾© optimizer å’Œæå¤±å‡½æ•¸
optimizer = torch.optim.Adam(model.parameters(),lr=2e-5)
criterion = nn.BCELoss()

# è¨“ç·´è¿´åœˆ

if __name__ == "__main__":
    if os.path.exists("model.pth"):
        print("âœ… å·²æ‰¾åˆ° model.pthï¼Œè¼‰å…¥æ¨¡å‹è·³éè¨“ç·´")
        model.load_state_dict(torch.load("model.pth", map_location=device))
    else:
        print("ğŸš€ æœªæ‰¾åˆ° model.pthï¼Œé–‹å§‹è¨“ç·´æ¨¡å‹...")
        num_epochs = 10
        for epoch in range(num_epochs):
            model.train()
            total_loss = 0.0
            for batch in train_loader:
                optimizer.zero_grad()
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                token_type_ids = batch["token_type_ids"].to(device)
                labels = batch["labels"].to(device)

                outputs = model(input_ids, attention_mask, token_type_ids)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"[Epoch{epoch+1}]Training Loss:{total_loss:.4f}")
        torch.save(model.state_dict(), "model.pth")# å„²å­˜æ¨¡å‹æ¬Šé‡
        print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆä¸¦å„²å­˜ç‚º model.pth")

